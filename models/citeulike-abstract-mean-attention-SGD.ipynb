{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "With your best friends; Cookie Optimizers :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CiteULike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\Anaconda2\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Michael\\Anaconda2\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Dataset, BucketIterator, Field, TabularDataset, Iterator\n",
    "from torchtext.vocab import Vocab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997\n"
     ]
    }
   ],
   "source": [
    "def condence(nparr):\n",
    "    uniq = np.unique(nparr)\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return np.array([name2idx[o] for o in nparr]), uniq\n",
    "\n",
    "## Read set\n",
    "text_information_index = 4\n",
    "max_len = 1000\n",
    "data='../datasets/citeulike/raw-data.csv'\n",
    "#interactions = '../datasets/citeulike/user-info-small.csv'\n",
    "interactions = '../datasets/citeulike/user-info.csv'\n",
    "rawtext = pd.read_csv(data)\n",
    "interactions = pd.read_csv(interactions)\n",
    "interactions[\"user.id\"], uniq = condence(interactions[\"user.id\"].values)\n",
    "sizes = [0.7, 0.2]\n",
    "\n",
    "interactions = interactions.sample(frac=1) # Shuffle\n",
    "filt = interactions[\"doc.id\"].map(lambda x: len(rawtext.iloc[x,text_information_index])) <= max_len\n",
    "interactions = interactions[filt]\n",
    "interactions[\"doc.id\"] = interactions[\"doc.id\"].transform(lambda idx: rawtext.iloc[int(idx),text_information_index])\n",
    "\n",
    "n = len(interactions)\n",
    "train_size = int(sizes[0] * n)\n",
    "val_size = int(sizes[1] * n)\n",
    "test_size = n - train_size - val_size\n",
    "\n",
    "train = interactions[:train_size]\n",
    "val = interactions[train_size:train_size+val_size]\n",
    "test = interactions[train_size+val_size:]\n",
    "\n",
    "n = len(train)\n",
    "\n",
    "uniq_items = np.unique(rawtext[\"doc.id\"])[:-1]\n",
    "uniq_users = np.unique(train[\"user.id\"])[:-1]\n",
    "items = set((x[0],x[1]) for x in train[[\"user.id\",\"doc.id\"]].values)\n",
    "\n",
    "pairs = []\n",
    "i = 0\n",
    "while(i < n):\n",
    "    \n",
    "    item = np.random.choice(uniq_items, size = 1)[0]\n",
    "    text = rawtext.iloc[int(item),text_information_index]\n",
    "    user = np.random.choice(uniq_users, size = 1)[0]\n",
    "    if len(text.split(' ')) <= max_len and (user,item) not in items:\n",
    "        i += 1\n",
    "        pairs += [(user,text,0)]\n",
    "        items.add((user,item))\n",
    "\n",
    "interactionsNegatives = np.vstack((train, pairs))\n",
    "\n",
    "train = pd.DataFrame(data = interactionsNegatives, columns = [\"user.id\",\"doc.id\", \"rating\"])\n",
    "\n",
    "val.to_csv('../datasets/citeulike/val.csv', header = False, index = False)\n",
    "test.to_csv('../datasets/citeulike/test.csv', header = False, index = False)\n",
    "\n",
    "del interactionsNegatives, items, uniq_items, uniq_users, pairs, interactions, val, test\n",
    "\n",
    "train = train.sample(frac=1) #shuffle panda style\n",
    "\n",
    "train.to_csv('../datasets/citeulike/train.csv', header = False, index = False)\n",
    "\n",
    "max_length = max(train[\"doc.id\"].map(lambda x: len(x.split(' '))))\n",
    "print(max_length)\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b6554e22fa53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspacy_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# create a tokenizer function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspacy_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'exists'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = Field(sequential=True, lower=True, include_lengths=True, fix_length=max_length)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "ID = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "        path='../datasets/citeulike', train='train.csv',\n",
    "        validation='val.csv', test='test.csv', format='csv',\n",
    "        fields=[('ID', ID), ('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)\n",
    "ID.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Text fields:')\n",
    "#print('keys of TEXT.vocab:', list(TEXT.vocab.__dict__.keys()))\n",
    "print(' size of vocabulary:', len(TEXT.vocab))\n",
    "print(\" vocabulary's embedding dimension:\", TEXT.vocab.vectors.size())\n",
    "print(' no. times the \"the\" appear in the dataset:', TEXT.vocab.freqs['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = (101, 102, 103)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes=batch_size, sort_key=lambda x: len(x.Text), sort_within_batch = True, repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embeddings\n",
    "TEXT_Shape = TEXT.vocab.vectors.size()\n",
    "embedding_dim = TEXT_Shape[1]\n",
    "num_items = TEXT_Shape[0]\n",
    "num_users = len(ID.vocab.itos)\n",
    "n_hidden = 104\n",
    "l1_hidden = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFNN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=embedding_dim, n_hidden=n_hidden, l1_hidden=l1_hidden):\n",
    "        super(CFNN, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, embedding_dim)\n",
    "        self.lin1 = nn.Linear(n_hidden+embedding_dim, l1_hidden)\n",
    "        self.lin2 = nn.Linear(l1_hidden, 1)\n",
    "        self.drop0 = nn.Dropout(0.3)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "                \n",
    "        # RNN decoding\n",
    "        self.rnn = nn.LSTM(embedding_dim, n_hidden, bidirectional = True)\n",
    "        self.rnnlin = nn.Linear(n_hidden*3, n_hidden)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        ## Attention module\n",
    "        self.attn = nn.Linear(n_hidden*2, n_hidden)\n",
    "        \n",
    "    def forward(self, u, v, hidden, seq_lengths):\n",
    "        #print(v.shape)\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "                \n",
    "        ## Packing, Encoding, Padding\n",
    "        packed = rnn_utils.pack_padded_sequence(V, seq_lengths).to(device)\n",
    "        rnnOut, (hn, cn) = self.rnn(packed, hidden)\n",
    "        padded, seq_lengths = rnn_utils.pad_packed_sequence(rnnOut, padding_value=0, total_length=max_length)\n",
    "        seq_lengths = seq_lengths.to(device).float()\n",
    "        \n",
    "        # Divide each batch_element by sequence_element and sum. (mean by seq_length)\n",
    "        padded = (padded.transpose(1,2) / seq_lengths).transpose(1,2)\n",
    "        mean = padded.sum(dim=0)\n",
    "        \n",
    "        V_rnn = torch.cat([mean, hn[-1]], dim=1)\n",
    "        V_rnn = F.relu(self.rnnlin(V_rnn))\n",
    "                 \n",
    "        x = torch.cat([U, V_rnn], dim=1)\n",
    "\n",
    "        ## Classification\n",
    "        x = self.drop0(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        init = torch.zeros(2, batch_size, n_hidden).to(device)\n",
    "        return (init,init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "def train(model, train_loader, optimizer, criterion, epoch, print_batch_p):\n",
    "    model.train()\n",
    "    \n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        users = data.ID\n",
    "        (items, seq_lengths) = data.Text\n",
    "        ratings = data.Label\n",
    "        batch_size = len(seq_lengths)\n",
    "        \n",
    "        users = users.long().to(device)\n",
    "        items = items.long().to(device)\n",
    "        ratings = ratings.float().to(device)      \n",
    "        \n",
    "        hidden_size = model.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(users, items, hidden_size, seq_lengths)\n",
    "        loss = criterion(output, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print jumping\n",
    "        percent = print_batch_p\n",
    "        proc = int((len(train_loader.dataset)/batch_size)*percent)\n",
    "        proc = proc if proc >= 1 else 1\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        TP,FP,TN,FN = 0,0,0,0\n",
    "        output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "        for y,yhat in zip(ratings.data, output_flat):\n",
    "            y = int(y)\n",
    "            if yhat == 0:\n",
    "                if y != yhat:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else:\n",
    "                if y != yhat:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "            \n",
    "        acc_list += [(TP + TN)/(TP+FP+TN+FN)]\n",
    "        loss_list += [loss.item()]\n",
    "        \n",
    "        if (batch_idx % proc == 0 and batch_idx != 0):\n",
    "            loss_mean = sum(loss_list)/len(loss_list)\n",
    "            acc_mean = sum(acc_list)/len(acc_list)\n",
    "            percent = 100 * (batch_idx / (len(train_loader)-1))\n",
    "            print(f'Train epoch {epoch:3d} ({percent:3.0f}%), Mean Accuracy: {acc_mean:2.5f}, Mean Loss: {loss_mean:2.5f}')\n",
    "            \n",
    "def validate(model, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0\n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "            users = data.ID\n",
    "            (items, seq_lengths) = data.Text\n",
    "            ratings = data.Label\n",
    "            batch_size = len(seq_lengths)\n",
    "            \n",
    "            users = users.long().to(device)\n",
    "            items = items.long().to(device)\n",
    "            ratings = ratings.float().to(device)\n",
    "            \n",
    "            hidden_size = model.init_hidden(batch_size)\n",
    "            output = model(users, items, hidden_size, seq_lengths)\n",
    "            loss = criterion(output.float(), ratings)\n",
    "            \n",
    "            output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "            for y,yhat in zip(ratings.data, output_flat):\n",
    "                y = int(y)\n",
    "                if yhat == 0:\n",
    "                    if y != yhat:\n",
    "                        FN += 1\n",
    "                    else:\n",
    "                        TN += 1\n",
    "                else:\n",
    "                    if y != yhat:\n",
    "                        FP += 1\n",
    "                    else:\n",
    "                        TP += 1\n",
    "            val_loss += loss.item() # sum up batch loss\n",
    "\n",
    "    acc = (TP + TN)/(TP + TN + FP + FN)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation epoch {epoch:>3d} (100%), Accucary: {acc:>2.5f}, Mean Loss: {val_loss:>2.5f} \\n')\n",
    "    return acc, val_loss\n",
    "\n",
    "def trainLoop(epochs, lr=0.001, wd = 1e-6, print_batch_p = 1, early_patience=5):\n",
    "    # Define model    \n",
    "    model = CFNN(num_users, num_items).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay = wd)\n",
    "    \n",
    "    accs = []\n",
    "    losses = []\n",
    "    best_acc = (-1,-1e16,1e16)\n",
    "    best_loss = (-1,-1e16,1e16)\n",
    "    early = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_iter, optimizer, criterion, epoch, print_batch_p)\n",
    "        val_acc, val_loss = validate(model, val_iter, criterion, epoch)\n",
    "        accs += [val_acc]\n",
    "        losses += [val_loss]\n",
    "        \n",
    "        early += 1\n",
    "        if val_acc > best_acc[-2]:\n",
    "            early = 0\n",
    "            best_acc = (epoch, val_acc, val_loss)\n",
    "        if val_loss < best_loss[-1]:\n",
    "            early = 0\n",
    "            best_loss = (epoch, val_acc, val_loss)\n",
    "        if early >= early_patience:\n",
    "            break\n",
    "        \n",
    "    plt.plot(range(1,epoch+1),accs)\n",
    "    plt.show()\n",
    "    print(f'best validatation accu epoch: {best_acc[0]:>3.0f}, acc: {best_acc[1]:>2.5f}, mean loss: {best_acc[2]:>2.5f} \\n')\n",
    "    plt.plot(range(1,epoch+1),losses)\n",
    "    plt.show()\n",
    "    print(f'best validatation loss epoch: {best_loss[0]:>3.0f}, acc: {best_loss[1]:>2.5f}, mean loss: {best_loss[2]:>2.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLoop(epochs=1000, lr=0.1, wd=1e-6, print_batch_p=0.33, early_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLoop(epochs=1000, lr=0.01, wd=1e-6, print_batch_p=0.33, early_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLoop(epochs=1000, lr=0.001, wd=1e-6, print_batch_p=0.33, early_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
