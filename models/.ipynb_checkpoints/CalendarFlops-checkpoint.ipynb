{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TalentFox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0 in /opt/conda/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /opt/conda/lib/python3.6/site-packages/de_core_news_sm -->\n",
      "    /opt/conda/lib/python3.6/site-packages/spacy/data/de\n",
      "\n",
      "    You can now load the model via spacy.load('de')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset, BucketIterator, Field, TabularDataset, Iterator\n",
    "from torchtext.vocab import Vocab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read set\n",
    "#data='../datasets/TalentFox/processed_dataset-small.csv'\n",
    "data='../datasets/TalentFox/processed_dataset.csv'\n",
    "\n",
    "parced_data = pd.read_csv(data)\n",
    "parced_data = parced_data[['candidate_resume','job_description', 'match_status']]\n",
    "parced_data['match_status'] = parced_data['match_status'][pd.to_numeric(parced_data['match_status'], errors='coerce').notnull()]\n",
    "parced_data = parced_data.dropna()\n",
    "\n",
    "#Flat\n",
    "parced_data['match_status'] = parced_data['match_status'].apply(lambda x: 0 if int(x) < 4 else 1)\n",
    "\n",
    "# Drop to equal\n",
    "parced_data = parced_data.sort_values(by=['match_status'], ascending=False)\n",
    "count = pd.value_counts(parced_data['match_status'].values, sort=False)\n",
    "count = count[1]\n",
    "parced_data = parced_data[:2*count]\n",
    "\n",
    "# Shuffle\n",
    "parced_data = parced_data.sample(frac=1)\n",
    "\n",
    "max_length_candidates = max((len(s.split(' ')) for s in parced_data['candidate_resume']))\n",
    "max_length_jobs = max((len(s.split(' ')) for s in parced_data['job_description']))\n",
    "\n",
    "sizes = [0.7, 0.2]\n",
    "n = len(parced_data)\n",
    "\n",
    "train_size = int(sizes[0] * n)\n",
    "val_size = int(sizes[1] * n)\n",
    "test_size = n - train_size - val_size\n",
    "\n",
    "train = parced_data[:train_size]\n",
    "val = parced_data[train_size:train_size+val_size]\n",
    "test = parced_data[train_size+val_size:]\n",
    "\n",
    "val.to_csv('../datasets/TalentFox/val.csv', header = False, index = False)\n",
    "test.to_csv('../datasets/TalentFox/test.csv', header = False, index = False)\n",
    "train.to_csv('../datasets/TalentFox/train.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "\n",
    "vec_url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.de.vec'\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "CANDIDATES = Field(sequential=True, lower=True, include_lengths=True, fix_length=max_length_candidates)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "JOBS = Field(sequential=True, lower=True, include_lengths=True, fix_length=max_length_jobs)\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "        path='../datasets/TalentFox', train='train.csv',\n",
    "        validation='val.csv', test='test.csv', format='csv',\n",
    "        fields=[('Candidates', CANDIDATES), ('Jobs', JOBS), ('Label', LABEL)])\n",
    "\n",
    "CANDIDATES.build_vocab(train, vectors=Vectors('wiki.de.vec', url=vec_url))\n",
    "LABEL.build_vocab(train)\n",
    "JOBS.build_vocab(train, vectors=Vectors('wiki.de.vec', url=vec_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text fields:\n",
      " Size of  job vocabulary: 8062\n",
      " Size of users vocabulary: 20075\n",
      " no. times the \"das\" appear in the dataset: 210\n",
      " Max length: Candidates: 3541, Jobs: 351\n"
     ]
    }
   ],
   "source": [
    "print('Text fields:')\n",
    "print(f' Size of  job vocabulary: {len(JOBS.vocab)}')\n",
    "print(f' Size of users vocabulary: {len(CANDIDATES.vocab)}')\n",
    "print(' no. times the \"das\" appear in the dataset:', JOBS.vocab.freqs['das']+CANDIDATES.vocab.freqs['das'])\n",
    "print(f' Max length: Candidates: {max_length_candidates}, Jobs: {max_length_jobs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = (100, 101, 102)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes = batch_size, sort_key=lambda x: len(x.Jobs), sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 20075, Number of jobs: 8062\n",
      "Candidates embedding dim 300\n",
      "Job embedding dim 300\n"
     ]
    }
   ],
   "source": [
    "# size of embeddings\n",
    "embedding_dim_candidates = CANDIDATES.vocab.vectors.size()[1]\n",
    "embedding_dim_jobs = JOBS.vocab.vectors.size()[1]\n",
    "num_jobs = JOBS.vocab.vectors.size()[0]\n",
    "num_candidates = CANDIDATES.vocab.vectors.size()[0]\n",
    "print(f'Number of candidates: {num_candidates}, Number of jobs: {num_jobs}')\n",
    "\n",
    "print(f'Candidates embedding dim {embedding_dim_candidates}')\n",
    "print(f'Job embedding dim {embedding_dim_jobs}')\n",
    "\n",
    "n_hidden = 91\n",
    "l1_hidden = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFNN(nn.Module):\n",
    "    def __init__(self, num_candidates,\n",
    "                 num_jobs,\n",
    "                 embedding_dim_candidates=embedding_dim_candidates,\n",
    "                 embedding_dim_jobs=embedding_dim_jobs,\n",
    "                 n_hidden=n_hidden,\n",
    "                 l1_hidden=l1_hidden):\n",
    "        super(CFNN, self).__init__()\n",
    "        \n",
    "        self.candidates_emb = nn.Embedding(num_candidates, embedding_dim_candidates)       \n",
    "        self.jobs_emb = nn.Embedding(max_length_candidates, max_length_jobs)\n",
    "               \n",
    "        self.lin1 = nn.Linear(max_length_candidates + num_candidates, l1_hidden)\n",
    "        self.lin2 = nn.Linear(l1_hidden, 1)\n",
    "        self.drop0 = nn.Dropout(0.1)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "                \n",
    "        ### RNN decoding\n",
    "        # Candidates\n",
    "        self.rnn_candidates = nn.LSTM(embedding_dim_candidates, n_hidden, batch_first = False)\n",
    "        self.rnnlin_candidates = nn.Linear(embedding_dim_candidates, n_hidden)\n",
    "        \n",
    "        # Jobs\n",
    "        self.rnn_jobs = nn.LSTM(embedding_dim_jobs, n_hidden, batch_first = False)\n",
    "        self.rnnlin_jobs = nn.Linear(100, n_hidden)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, c, j, hidden_candidates, hidden_jobs, candidates_length, job_length):\n",
    "        C = self.candidates_emb(c)\n",
    "        J = self.jobs_emb(j)\n",
    "        batch_size = len(candidates_length)\n",
    "        \n",
    "       \n",
    "        # Masking and meaning Candidates\n",
    "        mask_candidates = list()\n",
    "        for i in range(batch_size):\n",
    "            l = candidates_length.data[0].item()\n",
    "            m = [1]*l + [0]*(max_length_candidates-l)\n",
    "            mask_candidates += [m]       \n",
    "        mask_candidates = torch.from_numpy(np.array(mask_candidates)).to(device).float().transpose(0,1)             \n",
    "        mask_candidates = mask_candidates.unsqueeze(2)\n",
    "        \n",
    "        C = C * mask_candidates\n",
    "        C = C.mean(dim=2) #mean all embeddings\n",
    "               \n",
    "\n",
    "        # Masking and meaning Candidates\n",
    "        mask_jobs = list()\n",
    "        for i in range(batch_size):\n",
    "            l = job_length.data[0].item()\n",
    "            m = [1]*l + [0]*(max_length_jobs-l)\n",
    "            mask_jobs += [m]       \n",
    "        mask_jobs = torch.from_numpy(np.array(mask_jobs)).to(device).float().transpose(0,1)             \n",
    "        mask_jobs = mask_jobs.unsqueeze(2)\n",
    "        \n",
    "        J = J * mask_jobs\n",
    "        J = J.mean(dim=2) #mean all embeddings\n",
    "               \n",
    "        x = torch.cat([C, J], dim=0)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def init_hidden_candidates(self, batch_size):\n",
    "        init = torch.zeros(1, batch_size, n_hidden).to(device)\n",
    "        return (init,init)\n",
    "    \n",
    "    def init_hidden_jobs(self, batch_size):\n",
    "        init = torch.zeros(1, batch_size, n_hidden).to(device)\n",
    "        return (init,init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "def train(model, train_loader, optimizer, criterion, epoch, print_batch_p):\n",
    "    model.train()\n",
    "    \n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    loss_list = []\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        (candidates, candidates_length), (jobs, job_length), ratings = data\n",
    "                   \n",
    "        batch_s = len(candidates_length)\n",
    "        \n",
    "        candidates = candidates.long().to(device)\n",
    "        jobs = jobs.long().to(device)\n",
    "        ratings = ratings.float().to(device)      \n",
    "        ratings = ratings.view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_init_candidates = model.init_hidden_candidates(batch_s)\n",
    "        hidden_init_jobs = model.init_hidden_jobs(batch_s)\n",
    "        output = model(candidates, jobs, hidden_init_candidates, hidden_init_jobs, candidates_length, job_length)\n",
    "        output = output.view(-1)\n",
    "                     \n",
    "        loss = criterion(output, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print jumping\n",
    "        percent = print_batch_p\n",
    "        \n",
    "        proc = int((len(train_loader.dataset)/batch_s)*percent)\n",
    "        proc = proc if proc >= 1 else 1\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "\n",
    "        for y,yhat in zip(ratings.data, output_flat):\n",
    "            y = int(y)\n",
    "            if yhat == 0:\n",
    "                if y != yhat:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else:\n",
    "                if y != yhat:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "        loss_list += [loss.item()]\n",
    "        \n",
    "        if batch_idx % proc == 0 and batch_idx != 0:\n",
    "            loss_mean = sum(loss_list)/len(loss_list)\n",
    "            acc = (TP + TN)/(TP+FP+TN+FN)\n",
    "            \n",
    "            if TP + FN == 0:\n",
    "                recall = 0    \n",
    "            else:       \n",
    "                recall = TP/(TP + FN)\n",
    "                \n",
    "            if TP + FP == 0:\n",
    "                precision = 0\n",
    "            else:\n",
    "                precision = TP/(TP + FP)\n",
    "            \n",
    "            \n",
    "            if recall + precision == 0:\n",
    "                F1_score = 0\n",
    "            else:\n",
    "                F1_score = 2/(1/recall + 1/precision)\n",
    "            \n",
    "            #TP,FP,TN,FN = 0,0,0,0\n",
    "            #loss_list = []\n",
    "            print(f'Train epoch {epoch} ({100 * (batch_idx / len(train_loader)):.0f}%), Mean Loss: {loss_mean:.2f}, F1: {F1_score:.2f}')\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    outputlist = []\n",
    "    val_loss = 0\n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(val_loader):\n",
    "            (candidates, candidates_length), (jobs, job_length), ratings = data\n",
    "            batch_s = len(candidates_length)\n",
    "            \n",
    "            candidates = candidates.long().to(device)\n",
    "            jobs = jobs.long().to(device)\n",
    "            ratings = ratings.float().to(device)\n",
    "            \n",
    "            ratings = ratings.unsqueeze(1)\n",
    "            hidden_init_candidates = model.init_hidden_candidates(batch_s)\n",
    "            hidden_init_jobs = model.init_hidden_jobs(batch_s)\n",
    "            \n",
    "            output = model(candidates, jobs, hidden_init_candidates, hidden_init_jobs, candidates_length, job_length)\n",
    "            output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "            \n",
    "            for y,yhat in zip(ratings.data, output_flat):\n",
    "                y = int(y)\n",
    "                if yhat == 0:\n",
    "                    if y != yhat:\n",
    "                        FN += 1\n",
    "                    else:\n",
    "                        TN += 1\n",
    "                else:\n",
    "                    if y != yhat:\n",
    "                        FP += 1\n",
    "                    else:\n",
    "                        TP += 1\n",
    "        \n",
    "            outputlist += [output]\n",
    "            val_loss += criterion(output, ratings).item() # sum up batch loss\n",
    "\n",
    "    #print(TP, FN)\n",
    "    acc = (TP + TN)/(TP + TN + FP + FN)\n",
    "    \n",
    "    if TP + FN == 0:\n",
    "        recall = 0    \n",
    "    else:       \n",
    "        recall = TP/(TP + FN)\n",
    "\n",
    "    if TP + FP == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP/(TP + FP)\n",
    "\n",
    "    if recall + precision == 0:\n",
    "        F1_score = 0\n",
    "    else:\n",
    "        F1_score = 2/(1/recall + 1/precision)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch}: Validation average loss: {val_loss:.2f} | F1: {F1_score:.2f} | Accuracy: {acc:.2f} \\n' )\n",
    "    return acc, val_loss\n",
    "\n",
    "def trainLoop(epochs, lr=0.001, wd = 1e-6, print_batch_p = 1):\n",
    "    # Define model    \n",
    "    model = CFNN(num_candidates, num_jobs).to(device)\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay = wd)\n",
    "    \n",
    "    accs = []\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_iter, optimizer, criterion, epoch, print_batch_p)\n",
    "        acc, val_loss = validate(model, val_iter, criterion, epoch)\n",
    "        accs += [acc]\n",
    "        losses += [val_loss]\n",
    "        \n",
    "    plt.plot(range(1,epochs+1),accs)\n",
    "    plt.show()\n",
    "    plt.plot(range(1,epochs+1),losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532579805626/work/aten/src/THC/generic/THCTensorCopy.cpp:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9df45a59f5e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_batch_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-0f9f1461f532>\u001b[0m in \u001b[0;36mtrainLoop\u001b[0;34m(epochs, lr, wd, print_batch_p)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_batch_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0maccs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0f9f1461f532>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epoch, print_batch_p)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mhidden_init_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mhidden_init_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_init_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_init_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f6396f731970>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, c, j, hidden_candidates, hidden_jobs, candidates_length, job_length)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length_candidates\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mmask_candidates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mmask_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mmask_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532579805626/work/aten/src/THC/generic/THCTensorCopy.cpp:20"
     ]
    }
   ],
   "source": [
    "trainLoop(epochs=10, lr=0.1, wd=1e-6, print_batch_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
