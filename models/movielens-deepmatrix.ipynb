{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condence(nparr):\n",
    "    uniq = np.unique(nparr)\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return np.array([name2idx[o] for o in nparr])\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.rawdata = pd.read_csv(filename)\n",
    "        self.rawdata[\"userId\"] = condence(self.rawdata[\"userId\"].values)\n",
    "        self.rawdata[\"movieId\"] = condence(self.rawdata[\"movieId\"].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rawdata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx.item()\n",
    "        users = self.rawdata.iloc[idx, 0]\n",
    "        items = self.rawdata.iloc[idx, 1]\n",
    "        ratings = self.rawdata.iloc[idx, 2]\n",
    "        \n",
    "        return (users, items, ratings)\n",
    "    \n",
    "    def items (self):\n",
    "        n_users = self.rawdata[\"userId\"].nunique()\n",
    "        n_items = self.rawdata[\"movieId\"].nunique()\n",
    "        \n",
    "        return [n_users, n_items]\n",
    "\n",
    "def getLoaders(dataset, batchsize, shuffle, sizes):\n",
    "    train_size = int(sizes[0] * len(dataset))\n",
    "    val_size = int(sizes[1] * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    return [DataLoader(data, batch_size = batchsize, shuffle = shuffle) for data in [train_data, val_data, test_data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class CFNN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size, n_hidden_1, n_hidden_2, p_dropout):\n",
    "        super(CFNN, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.lin1 = nn.Linear(emb_size*2, n_hidden_1)\n",
    "        self.lin2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.linOut = nn.Linear(n_hidden_2, 1)\n",
    "        self.drop0 = nn.Dropout(p_dropout)\n",
    "        self.drop1 = nn.Dropout(p_dropout)\n",
    "        self.drop2 = nn.Dropout(p_dropout)\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "        x = F.relu(torch.cat([U, V], dim=1))\n",
    "        x = self.drop0(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.linOut(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for (users, items, ratings) in train_loader:\n",
    "        users = users.long().to(device)\n",
    "        items = items.long().to(device)\n",
    "        ratings = ratings.float().to(device)\n",
    "        ratings = ratings.unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(users, items)\n",
    "        loss = criterion(output, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    return train_loss / len(train_loader)\n",
    "    \n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (users, items, ratings) in val_loader:\n",
    "            users = users.long().to(device)\n",
    "            items = items.long().to(device)\n",
    "            ratings = ratings.float().to(device)\n",
    "            ratings = ratings.unsqueeze(1)\n",
    "            \n",
    "            output = model(users, items)\n",
    "            loss = criterion(output, ratings)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "def train_model(train_loader, val_loader, max_epochs, batchsize, model, criterion, optimizer, modelpath=None):\n",
    "    # check path for early stopping\n",
    "    if modelpath != None:\n",
    "        patience_count = 0\n",
    "        torch.save(model, modelpath)\n",
    "    \n",
    "    # Train model\n",
    "    best_loss = (-1, -1, 1e15)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # train and validate\n",
    "        train_loss = train(model, train_loader, optimizer, criterion)\n",
    "        val_loss = validate(model, val_loader, criterion)\n",
    "        \n",
    "        val_losses += [val_loss]\n",
    "        train_losses += [train_loss]\n",
    "        print(f'Epoch {epoch:3d} | Train average loss: {train_loss:3.5f} | Validation average loss: {val_loss:3.5f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_loss[2]:\n",
    "            best_loss = (epoch, train_loss, val_loss)\n",
    "            \n",
    "            if modelpath != None:\n",
    "                patience_count = 0\n",
    "                torch.save(model, modelpath)\n",
    "        if modelpath != None:\n",
    "            if patience_count == 10:\n",
    "                model = torch.load(modelpath)\n",
    "                break\n",
    "            else:\n",
    "                patience_count += 1\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(range(1,epoch+1), val_losses, color='b')\n",
    "    plt.plot(range(1,epoch+1), train_losses, color='g')\n",
    "    plt.axvline(x=best_loss[0], color='r')\n",
    "    plt.show()\n",
    "    print(f\"Best validation loss at epoch {best_loss[0]} | Train loss: {best_loss[1]:.5} | Validation loss {best_loss[2]:.5}\")\n",
    "    \n",
    "    # return loaded model if early stop otherwise, finished model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "datasetpath = '../datasets/movielens-large/ratings.csv'\n",
    "#datasetpath = '../datasets/movielens-small/ratings.csv'\n",
    "shuffle_data = True\n",
    "size_splits = [0.7, 0.2, 0.1]\n",
    "dataset = MovieLensDataset(datasetpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "n_users, n_items = dataset.items()\n",
    "emb_size=100\n",
    "n_hidden_1=1000\n",
    "n_hidden_2=1000\n",
    "p_dropout = 0.3\n",
    "model = CFNN(n_users, n_items, emb_size, n_hidden_1, n_hidden_2, p_dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "modelpath = '../torch_models/movielens/dmf_best.pth'\n",
    "max_epochs = 100\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-6\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader, val_loader, test_loader = getLoaders(dataset, batch_size, shuffle_data, size_splits)\n",
    "\n",
    "# Print info on model and loaders\n",
    "ls = [len(l) for l in [train_loader, val_loader, test_loader]]\n",
    "mbls = max([len(str(l)) for l in ls])\n",
    "mls = max([len(str(l*batch_size)) for l in ls])\n",
    "\n",
    "print(model, \"\\n\")\n",
    "print(f\"Amount of batches in training set:   {ls[0]:{mbls}d} | Total samples: {ls[0]*batch_size:{mls}d}\")\n",
    "print(f\"Amount of batches in validation set: {ls[1]:{mbls}d} | Total samples: {ls[1]*batch_size:{mls}d}\")\n",
    "print(f\"Amount of batches in testing set:    {ls[2]:{mbls}d} | Total samples: {ls[2]*batch_size:{mls}d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = train_model(train_loader,\n",
    "                            val_loader, \n",
    "                            max_epochs, \n",
    "                            batch_size, \n",
    "                            model, \n",
    "                            criterion, \n",
    "                            optimizer, \n",
    "                            modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for (users, items, ratings) in test_loader:\n",
    "        users = users.long().to(device)\n",
    "        items = items.long().to(device)\n",
    "        ratings = ratings.float().to(device)\n",
    "        ratings = ratings.unsqueeze(1)\n",
    "        output = model_trained(users, items)\n",
    "\n",
    "        test_loss += criterion(output, ratings).item() # sum up batch loss\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "# UNCOMMENT ONLY WHEN MODEL IS COMPLETELY FINISHED\n",
    "print(f'Final test average loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
