{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CalendarFlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0 in c:\\users\\micha\\appdata\\local\\conda\\conda\\envs\\deeplearning-02456\\lib\\site-packages (2.0.0)\n",
      "\n",
      "    Linking successful\n",
      "    C:\\Users\\Micha\\AppData\\Local\\conda\\conda\\envs\\deeplearning-02456\\lib\\site-packages\\de_core_news_sm\n",
      "    -->\n",
      "    C:\\Users\\Micha\\AppData\\Local\\conda\\conda\\envs\\deeplearning-02456\\lib\\site-packages\\spacy\\data\\de\n",
      "\n",
      "    You can now load the model via spacy.load('de')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset, BucketIterator, Field, TabularDataset, Iterator\n",
    "from torchtext.vocab import Vocab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read set\n",
    "#data='../datasets/TalentFox/processed_dataset-small.csv'\n",
    "data='../datasets/TalentFox/processed_dataset.csv'\n",
    "\n",
    "parced_data = pd.read_csv(data)\n",
    "parced_data = parced_data[['candidate_resume','job_description', 'match_status']]\n",
    "parced_data['match_status'] = parced_data['match_status'][pd.to_numeric(parced_data['match_status'], errors='coerce').notnull()]\n",
    "parced_data = parced_data.dropna()\n",
    "\n",
    "#Flat\n",
    "parced_data['match_status'] = parced_data['match_status'].apply(lambda x: 0 if int(x) < 4 else 1)\n",
    "\n",
    "# Max length\n",
    "min_len = 100\n",
    "filt_candidate = parced_data[\"candidate_resume\"].map(lambda x: len(x)) >= min_len\n",
    "parced_data = parced_data[filt_candidate]\n",
    "filt_jobs = parced_data[\"job_description\"].map(lambda x: len(x)) <= max_len\n",
    "parced_data = parced_data[filt_jobs]\n",
    "\n",
    "# Drop to equal\n",
    "parced_data = parced_data.sort_values(by=['match_status'], ascending=False)\n",
    "count = pd.value_counts(parced_data['match_status'].values, sort=False)\n",
    "count = count[1]\n",
    "parced_data = parced_data[:2*count]\n",
    "\n",
    "# Shuffle\n",
    "parced_data = parced_data.sample(frac=1)\n",
    "\n",
    "max_length_candidates = max((len(s.split(' ')) for s in parced_data['candidate_resume']))\n",
    "max_length_jobs = max((len(s.split(' ')) for s in parced_data['job_description']))\n",
    "\n",
    "sizes = [0.7, 0.2]\n",
    "n = len(parced_data)\n",
    "\n",
    "train_size = int(sizes[0] * n)\n",
    "val_size = int(sizes[1] * n)\n",
    "test_size = n - train_size - val_size\n",
    "\n",
    "train = parced_data[:train_size]\n",
    "val = parced_data[train_size:train_size+val_size]\n",
    "test = parced_data[train_size+val_size:]\n",
    "\n",
    "val.to_csv('../datasets/TalentFox/val.csv', header = False, index = False)\n",
    "test.to_csv('../datasets/TalentFox/test.csv', header = False, index = False)\n",
    "train.to_csv('../datasets/TalentFox/train.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/2275234 [00:00<?, ?it/s]Skipping token 2275233 with 1-dimensional vector ['300']; likely a header\n",
      "100%|██████████████████████████████████████████████████████████████████████| 2275234/2275234 [04:26<00:00, 8521.90it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "\n",
    "vec_url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.de.vec'\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "CANDIDATES = Field(sequential=True, lower=True, include_lengths=True, fix_length=max_length_candidates, tokenize=tokenizer)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "JOBS = Field(sequential=True, lower=True, include_lengths=True, fix_length=max_length_jobs, tokenize=tokenizer)\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "        path='../datasets/TalentFox', train='train.csv',\n",
    "        validation='val.csv', test='test.csv', format='csv',\n",
    "        fields=[('Candidates', CANDIDATES), ('Jobs', JOBS), ('Label', LABEL)])\n",
    "\n",
    "CANDIDATES.build_vocab(train, vectors=Vectors('wiki.de.vec', url=vec_url))\n",
    "LABEL.build_vocab(train)\n",
    "JOBS.build_vocab(train, vectors=Vectors('wiki.de.vec', url=vec_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Text fields:')\n",
    "print(f' Size of  job vocabulary: {len(JOBS.vocab)}')\n",
    "print(f' Size of users vocabulary: {len(CANDIDATES.vocab)}')\n",
    "print(' no. times the \"das\" appear in the dataset:', JOBS.vocab.freqs['das']+CANDIDATES.vocab.freqs['das'])\n",
    "print(f' Max length: Candidates: {max_length_candidates}, Jobs: {max_length_jobs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = (10, 11, 12)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes = batch_size, sort_key=lambda x: len(x.Jobs), sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embeddings\n",
    "embedding_dim_candidates = CANDIDATES.vocab.vectors.size()[1]\n",
    "embedding_dim_jobs = JOBS.vocab.vectors.size()[1]\n",
    "num_jobs = JOBS.vocab.vectors.size()[0]\n",
    "num_candidates = CANDIDATES.vocab.vectors.size()[0]\n",
    "print(f'Number of candidates: {num_candidates}, Number of jobs: {num_jobs}')\n",
    "\n",
    "print(f'Candidates embedding dim {embedding_dim_candidates}')\n",
    "print(f'Job embedding dim {embedding_dim_jobs}')\n",
    "\n",
    "n_hidden = 91\n",
    "l1_hidden = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFNN(nn.Module):\n",
    "    def __init__(self, num_candidates,\n",
    "                 num_jobs,\n",
    "                 embedding_dim_candidates=embedding_dim_candidates,\n",
    "                 embedding_dim_jobs=embedding_dim_jobs,\n",
    "                 n_hidden=n_hidden,\n",
    "                 l1_hidden=l1_hidden):\n",
    "        super(CFNN, self).__init__()\n",
    "        \n",
    "        self.candidates_emb = nn.Embedding(num_candidates, embedding_dim_candidates)       \n",
    "        self.jobs_emb = nn.Embedding(num_jobs, embedding_dim_jobs)\n",
    "               \n",
    "        self.lin1 = nn.Linear(embedding_dim_candidates + embedding_dim_jobs, l1_hidden)\n",
    "        self.lin2 = nn.Linear(l1_hidden, 1)\n",
    "        self.drop0 = nn.Dropout(0.1)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "                \n",
    "        ### RNN decoding\n",
    "        # Candidates\n",
    "        self.rnn_candidates = nn.LSTM(embedding_dim_candidates, n_hidden, batch_first = False)\n",
    "        self.rnnlin_candidates = nn.Linear(embedding_dim_candidates, n_hidden)\n",
    "        \n",
    "        # Jobs\n",
    "        self.rnn_jobs = nn.LSTM(embedding_dim_jobs, n_hidden, batch_first = False)\n",
    "        self.rnnlin_jobs = nn.Linear(100, n_hidden)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, c, j, hidden_candidates, hidden_jobs, candidates_length, job_length):\n",
    "        C = self.candidates_emb(c)\n",
    "        J = self.jobs_emb(j)\n",
    "        batch_size = len(candidates_length)\n",
    "        \n",
    "        # Masking and meaning Candidates\n",
    "        mask_candidates = list()\n",
    "        for i in range(batch_size):\n",
    "            l = candidates_length.data[0].item()\n",
    "            m = [1]*l + [0]*(max_length_candidates-l)\n",
    "            mask_candidates += [m]       \n",
    "        mask_candidates = torch.from_numpy(np.array(mask_candidates)).to(device).float().transpose(0,1)             \n",
    "        mask_candidates = mask_candidates.unsqueeze(2)\n",
    "        \n",
    "        C = C * mask_candidates\n",
    "        C = C.sum(0)/candidates_length.unsqueeze(1).float().to(device)\n",
    "\n",
    "        # Masking and meaning Jobs\n",
    "        ## Packing, Encoding, Padding\n",
    "        packed = rnn_utils.pack_padded_sequence(J, job_length).to(device)\n",
    "        rnnOut, (hn, cn) = self.rnn_jobs(packed, hidden_jobs)\n",
    "        #padded, seq_lengths = rnn_utils.pad_packed_sequence(rnnOut, padding_value=0, total_length=max_length)\n",
    "        #seq_lengths = seq_lengths.to(device).float()\n",
    "        J = hn.unsqueeze(1)\n",
    "        J = self.rnnlin_jobs(J)\n",
    "        #mask_jobs = list()\n",
    "        #for i in range(batch_size):\n",
    "        #    l = job_length.data[0].item()\n",
    "        #    m = [1]*l + [0]*(max_length_jobs-l)\n",
    "        #    mask_jobs += [m]       \n",
    "        #mask_jobs = torch.from_numpy(np.array(mask_jobs)).to(device).float().transpose(0,1)\n",
    "        #mask_jobs = mask_jobs.unsqueeze(2)\n",
    "        \n",
    "        #J = J * mask_jobs\n",
    "        #J = J.sum(0)/job_length.unsqueeze(1).float().to(device)\n",
    "        \n",
    "        x = torch.cat([C, J], dim=1)\n",
    "        \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def init_hidden_candidates(self, batch_size):\n",
    "        init = torch.zeros(1, batch_size, n_hidden).to(device)\n",
    "        return (init,init)\n",
    "    \n",
    "    def init_hidden_jobs(self, batch_size):\n",
    "        init = torch.zeros(1, batch_size, n_hidden).to(device)\n",
    "        return (init,init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "def train(model, train_loader, optimizer, criterion, epoch, print_batch_p):\n",
    "    model.train()\n",
    "    \n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    loss_list = []\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        (candidates, candidates_length), (jobs, job_length), ratings = data\n",
    "                   \n",
    "        batch_s = len(candidates_length)\n",
    "        \n",
    "        candidates = candidates.long().to(device)\n",
    "        jobs = jobs.long().to(device)\n",
    "        ratings = ratings.float().to(device)      \n",
    "        ratings = ratings.view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_init_candidates = model.init_hidden_candidates(batch_s)\n",
    "        hidden_init_jobs = model.init_hidden_jobs(batch_s)\n",
    "        output = model(candidates, jobs, hidden_init_candidates, hidden_init_jobs, candidates_length, job_length)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        loss = criterion(output, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print jumping\n",
    "        percent = print_batch_p\n",
    "        \n",
    "        proc = int((len(train_loader.dataset)/batch_s)*percent)\n",
    "        proc = proc if proc >= 1 else 1\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "\n",
    "        for y,yhat in zip(ratings.data, output_flat):\n",
    "            y = int(y)\n",
    "            if yhat == 0:\n",
    "                if y != yhat:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else:\n",
    "                if y != yhat:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "        loss_list += [loss.item()]\n",
    "        \n",
    "        if batch_idx % proc == 0 and batch_idx != 0:\n",
    "            loss_mean = sum(loss_list)/len(loss_list)\n",
    "            acc = (TP + TN)/(TP+FP+TN+FN)\n",
    "            \n",
    "            if TP + FN == 0:\n",
    "                recall = 0    \n",
    "            else:       \n",
    "                recall = TP/(TP + FN)\n",
    "                \n",
    "            if TP + FP == 0:\n",
    "                precision = 0\n",
    "            else:\n",
    "                precision = TP/(TP + FP)\n",
    "            \n",
    "            \n",
    "            if recall + precision == 0:\n",
    "                F1_score = 0\n",
    "            else:\n",
    "                F1_score = 2/(1/recall + 1/precision)\n",
    "            \n",
    "            #TP,FP,TN,FN = 0,0,0,0\n",
    "            #loss_list = []\n",
    "            print(f'Train epoch {epoch} ({100 * (batch_idx / len(train_loader)):.0f}%), Mean Loss: {loss_mean:.2f}, F1: {F1_score:.2f}')\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    outputlist = []\n",
    "    val_loss = 0\n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(val_loader):\n",
    "            (candidates, candidates_length), (jobs, job_length), ratings = data\n",
    "            batch_s = len(candidates_length)\n",
    "            \n",
    "            candidates = candidates.long().to(device)\n",
    "            jobs = jobs.long().to(device)\n",
    "            ratings = ratings.float().to(device)\n",
    "            \n",
    "            ratings = ratings.unsqueeze(1)\n",
    "            hidden_init_candidates = model.init_hidden_candidates(batch_s)\n",
    "            hidden_init_jobs = model.init_hidden_jobs(batch_s)\n",
    "            \n",
    "            output = model(candidates, jobs, hidden_init_candidates, hidden_init_jobs, candidates_length, job_length)\n",
    "            output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "            \n",
    "            for y,yhat in zip(ratings.data, output_flat):\n",
    "                y = int(y)\n",
    "                if yhat == 0:\n",
    "                    if y != yhat:\n",
    "                        FN += 1\n",
    "                    else:\n",
    "                        TN += 1\n",
    "                else:\n",
    "                    if y != yhat:\n",
    "                        FP += 1\n",
    "                    else:\n",
    "                        TP += 1\n",
    "        \n",
    "            outputlist += [output]\n",
    "            val_loss += criterion(output, ratings).item() # sum up batch loss\n",
    "\n",
    "    #print(TP, FN)\n",
    "    acc = (TP + TN)/(TP + TN + FP + FN)\n",
    "    \n",
    "    if TP + FN == 0:\n",
    "        recall = 0    \n",
    "    else:       \n",
    "        recall = TP/(TP + FN)\n",
    "\n",
    "    if TP + FP == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP/(TP + FP)\n",
    "\n",
    "    if recall + precision == 0:\n",
    "        F1_score = 0\n",
    "    else:\n",
    "        F1_score = 2/(1/recall + 1/precision)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch}: Validation average loss: {val_loss:.2f} | F1: {F1_score:.2f} | Accuracy: {acc:.2f} \\n' )\n",
    "    return acc, val_loss\n",
    "\n",
    "def trainLoop(epochs, lr=0.001, wd = 1e-6, print_batch_p = 1):\n",
    "    # Define model    \n",
    "    model = CFNN(num_candidates, num_jobs).to(device)\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay = wd)\n",
    "    \n",
    "    accs = []\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_iter, optimizer, criterion, epoch, print_batch_p)\n",
    "        acc, val_loss = validate(model, val_iter, criterion, epoch)\n",
    "        accs += [acc]\n",
    "        losses += [val_loss]\n",
    "        \n",
    "    plt.plot(range(1,epochs+1),accs)\n",
    "    plt.show()\n",
    "    plt.plot(range(1,epochs+1),losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoop(epochs=10, lr=0.001, wd=1e-6, print_batch_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
