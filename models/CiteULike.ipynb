{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "With your best friends; Cookie Optimizers :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CiteULike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /opt/conda/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /opt/conda/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /opt/conda/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset, BucketIterator, Field, TabularDataset, Iterator\n",
    "from torchtext.vocab import Vocab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    }
   ],
   "source": [
    "def condence(nparr):\n",
    "    uniq = np.unique(nparr)\n",
    "    name2idx = {o:i for i,o in enumerate(uniq)}\n",
    "    return np.array([name2idx[o] for o in nparr]), uniq\n",
    "\n",
    "## Read set\n",
    "text_information_index = 4\n",
    "max_len = 1000\n",
    "data='../datasets/citeulike/raw-data.csv'\n",
    "interactions = '../datasets/citeulike/user-info-small.csv'\n",
    "#interactions = '../datasets/citeulike/user-info.csv'\n",
    "rawtext = pd.read_csv(data)\n",
    "interactions = pd.read_csv(interactions)\n",
    "interactions[\"user.id\"], uniq = condence(interactions[\"user.id\"].values)\n",
    "sizes = [0.7, 0.2]\n",
    "\n",
    "interactions = interactions.sample(frac=1) # Shuffle\n",
    "filt = interactions[\"doc.id\"].map(lambda x: len(rawtext.iloc[x,text_information_index])) <= max_len\n",
    "interactions = interactions[filt]\n",
    "interactions[\"doc.id\"] = interactions[\"doc.id\"].transform(lambda idx: rawtext.iloc[int(idx),text_information_index])\n",
    "\n",
    "n = len(interactions)\n",
    "train_size = int(sizes[0] * n)\n",
    "val_size = int(sizes[1] * n)\n",
    "test_size = n - train_size - val_size\n",
    "\n",
    "train = interactions[:train_size]\n",
    "val = interactions[train_size:train_size+val_size]\n",
    "test = interactions[train_size+val_size:]\n",
    "\n",
    "n = len(train)\n",
    "\n",
    "uniq_items = np.unique(rawtext[\"doc.id\"])[:-1]\n",
    "uniq_users = np.unique(train[\"user.id\"])[:-1]\n",
    "items = set((x[0],x[1]) for x in train[[\"user.id\",\"doc.id\"]].values)\n",
    "\n",
    "pairs = []\n",
    "i = 0\n",
    "while(i < n):\n",
    "    \n",
    "    item = np.random.choice(uniq_items, size = 1)[0]\n",
    "    text = rawtext.iloc[int(item),text_information_index]\n",
    "    user = np.random.choice(uniq_users, size = 1)[0]\n",
    "    if len(text.split(' ')) <= max_len and (user,item) not in items:\n",
    "        i += 1\n",
    "        pairs += [(user,text,0)]\n",
    "        items.add((user,item))\n",
    "\n",
    "interactionsNegatives = np.vstack((train, pairs))\n",
    "\n",
    "train = pd.DataFrame(data = interactionsNegatives, columns = [\"user.id\",\"doc.id\", \"rating\"])\n",
    "\n",
    "val.to_csv('../datasets/citeulike/val.csv', header = False, index = False)\n",
    "test.to_csv('../datasets/citeulike/test.csv', header = False, index = False)\n",
    "\n",
    "del interactionsNegatives, items, uniq_items, uniq_users, pairs, interactions, val, test\n",
    "\n",
    "train = train.sample(frac=1) #shuffle panda style\n",
    "\n",
    "train.to_csv('../datasets/citeulike/train.csv', header = False, index = False)\n",
    "\n",
    "max_length = max(train[\"doc.id\"].map(lambda x: len(x.split(' '))))\n",
    "print(max_length)\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = Field(sequential=True, lower=True, include_lengths=True, fix_length=max_length)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "ID = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "        path='../datasets/citeulike', train='train.csv',\n",
    "        validation='val.csv', test='test.csv', format='csv',\n",
    "        fields=[('ID', ID), ('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)\n",
    "ID.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text fields:\n",
      " size of vocabulary: 22698\n",
      " vocabulary's embedding dimension: torch.Size([22698, 100])\n",
      " no. times the \"the\" appear in the dataset: 10049\n"
     ]
    }
   ],
   "source": [
    "print('Text fields:')\n",
    "#print('keys of TEXT.vocab:', list(TEXT.vocab.__dict__.keys()))\n",
    "print(' size of vocabulary:', len(TEXT.vocab))\n",
    "print(\" vocabulary's embedding dimension:\", TEXT.vocab.vectors.size())\n",
    "print(' no. times the \"the\" appear in the dataset:', TEXT.vocab.freqs['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = (101, 102, 103)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes=batch_size, sort_key=lambda x: len(x.Text), sort_within_batch = True, repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    5,   991,     2,  9274,  2657, 10365, 10365, 12734,  1059,  1059,\n",
      "         1664,    19,     7,     7,   991,   991,   991,  2299,     2,    14,\n",
      "          991,     7,     2,    11,  2677,   123,  1407,    46,   259,     2,\n",
      "         1059,     7,  1664,  1059,    11,    11,     7,   965, 11333,  2569,\n",
      "           58,   991,   991,  2677,     2,  1664,    26,   301, 11338,    46,\n",
      "         3097,    13,  2182,    19,    11,   106,   236,     2, 22538, 11379,\n",
      "        10379, 10379, 11331,   177, 11380, 12732,    90,  3889,     6,     2,\n",
      "        12735,  1059,   608,   498,   828,  1371,  5351,  6557,     2,  1664,\n",
      "            2,  2677, 11334,     7,    63, 11376,  1082,  4783, 10844,  1059,\n",
      "          924,    28,   219,     2,     2,  1641,    11, 22485,   202,   386,\n",
      "         5773])\n"
     ]
    }
   ],
   "source": [
    "batch, sl = next(enumerate(train_iter))[1].Text\n",
    "print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101]) torch.Size([859, 101])\n"
     ]
    }
   ],
   "source": [
    "print(sl.shape, batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embeddings\n",
    "TEXT_Shape = TEXT.vocab.vectors.size()\n",
    "embedding_dim = TEXT_Shape[1]\n",
    "num_items = TEXT_Shape[0]\n",
    "num_users = len(ID.vocab.itos)\n",
    "n_hidden = 104\n",
    "l1_hidden = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFNN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=embedding_dim, n_hidden=n_hidden, l1_hidden=l1_hidden):\n",
    "        super(CFNN, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, embedding_dim)\n",
    "        self.lin1 = nn.Linear(n_hidden+embedding_dim, l1_hidden)\n",
    "        self.lin2 = nn.Linear(l1_hidden, 1)\n",
    "        self.drop0 = nn.Dropout(0.3)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "                \n",
    "        # RNN decoding\n",
    "        self.rnn = nn.LSTM(embedding_dim, n_hidden, bidirectional = True)\n",
    "        self.rnnlin = nn.Linear(n_hidden*3, n_hidden)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        ## Attention module\n",
    "        self.attn = nn.Linear(n_hidden*2, n_hidden)\n",
    "        \n",
    "    def forward(self, u, v, hidden, seq_lengths):\n",
    "        #print(v.shape)\n",
    "        U = self.user_emb(u)\n",
    "        V = self.item_emb(v)\n",
    "                \n",
    "        ## Packing, Encoding, Padding\n",
    "        packed = rnn_utils.pack_padded_sequence(V, seq_lengths).to(device)\n",
    "        rnnOut, (hn, cn) = self.rnn(packed, hidden)\n",
    "        padded, seq_lengths = rnn_utils.pad_packed_sequence(rnnOut, padding_value=0, total_length=max_length)\n",
    "        seq_lengths = seq_lengths.to(device).float()\n",
    "        \n",
    "        # Divide each batch_element by sequence_element and sum. (mean by seq_length)\n",
    "        padded = (padded.transpose(1,2) / seq_lengths).transpose(1,2)\n",
    "        mean = padded.sum(dim=0)\n",
    "        \n",
    "        V_rnn = torch.cat([mean, hn[-1]], dim=1)\n",
    "        V_rnn = F.relu(self.rnnlin(V_rnn))\n",
    "                 \n",
    "        x = torch.cat([U, V_rnn], dim=1)\n",
    "\n",
    "        ## Classification\n",
    "        x = self.drop0(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        init = torch.zeros(2, batch_size, n_hidden).to(device)\n",
    "        return (init,init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "def train(model, train_loader, optimizer, criterion, epoch, print_batch_p):\n",
    "    model.train()\n",
    "    \n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        users = data.ID\n",
    "        (items, seq_lengths) = data.Text\n",
    "        ratings = data.Label\n",
    "        batch_size = len(seq_lengths)\n",
    "        \n",
    "        users = users.long().to(device)\n",
    "        items = items.long().to(device)\n",
    "        ratings = ratings.float().to(device)      \n",
    "        \n",
    "        hidden_size = model.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(users, items, hidden_size, seq_lengths)\n",
    "        loss = criterion(output, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print jumping\n",
    "        percent = print_batch_p\n",
    "        proc = int((len(train_loader.dataset)/batch_size)*percent)\n",
    "        proc = proc if proc >= 1 else 1\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        TP,FP,TN,FN = 0,0,0,0\n",
    "        output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "        for y,yhat in zip(ratings.data, output_flat):\n",
    "            y = int(y)\n",
    "            if yhat == 0:\n",
    "                if y != yhat:\n",
    "                    FN += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else:\n",
    "                if y != yhat:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "            \n",
    "        acc_list += [(TP + TN)/(TP+FP+TN+FN)]\n",
    "        loss_list += [loss.item()]\n",
    "        \n",
    "        if (batch_idx % proc == 0 and batch_idx != 0):\n",
    "            loss_mean = sum(loss_list)/len(loss_list)\n",
    "            acc_mean = sum(acc_list)/len(acc_list)\n",
    "            percent = 100 * (batch_idx / (len(train_loader)-1))\n",
    "            print(f'Train epoch {epoch:3d} ({percent:3.0f}%), Mean Accuracy: {acc_mean:2.5f}, Mean Loss: {loss_mean:2.5f}')\n",
    "            \n",
    "def validate(model, val_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0\n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(val_loader):\n",
    "            users = data.ID\n",
    "            (items, seq_lengths) = data.Text\n",
    "            ratings = data.Label\n",
    "            batch_size = len(seq_lengths)\n",
    "            \n",
    "            users = users.long().to(device)\n",
    "            items = items.long().to(device)\n",
    "            ratings = ratings.float().to(device)\n",
    "            \n",
    "            hidden_size = model.init_hidden(batch_size)\n",
    "            output = model(users, items, hidden_size, seq_lengths)\n",
    "            loss = criterion(output.float(), ratings)\n",
    "            \n",
    "            output_flat = [0 if o < 0.5 else 1 for o in output.data]\n",
    "            for y,yhat in zip(ratings.data, output_flat):\n",
    "                y = int(y)\n",
    "                if yhat == 0:\n",
    "                    if y != yhat:\n",
    "                        FN += 1\n",
    "                    else:\n",
    "                        TN += 1\n",
    "                else:\n",
    "                    if y != yhat:\n",
    "                        FP += 1\n",
    "                    else:\n",
    "                        TP += 1\n",
    "            val_loss += loss.item() # sum up batch loss\n",
    "\n",
    "    acc = (TP + TN)/(TP + TN + FP + FN)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation epoch {epoch:>3d} (100%), Accucary: {acc:>2.5f}, Mean Loss: {val_loss:>2.5f} \\n')\n",
    "    return acc, val_loss\n",
    "\n",
    "def trainLoop(epochs, lr=0.001, wd = 1e-6, print_batch_p = 1, early_patience=5):\n",
    "    # Define model    \n",
    "    model = CFNN(num_users, num_items).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay = wd)\n",
    "    \n",
    "    accs = []\n",
    "    losses = []\n",
    "    best_acc = (-1,-1e16,1e16)\n",
    "    best_loss = (-1,-1e16,1e16)\n",
    "    early = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_iter, optimizer, criterion, epoch, print_batch_p)\n",
    "        val_acc, val_loss = validate(model, val_iter, criterion, epoch)\n",
    "        accs += [val_acc]\n",
    "        losses += [val_loss]\n",
    "        \n",
    "        early += 1\n",
    "        if val_acc > best_acc[-2]:\n",
    "            early = 0\n",
    "            best_acc = (epoch, val_acc, val_loss)\n",
    "        if val_loss < best_loss[-1]:\n",
    "            early = 0\n",
    "            best_loss = (epoch, val_acc, val_loss)\n",
    "        if early >= early_patience:\n",
    "            break\n",
    "        \n",
    "    plt.plot(range(1,epoch+1),accs)\n",
    "    plt.show()\n",
    "    print(f'best validatation accu epoch: {best_acc[0]:>3.0f}, acc: {best_acc[1]:>2.5f}, mean loss: {best_acc[2]:>2.5f} \\n')\n",
    "    plt.plot(range(1,epoch+1),losses)\n",
    "    plt.show()\n",
    "    print(f'best validatation loss epoch: {best_loss[0]:>3.0f}, acc: {best_loss[1]:>2.5f}, mean loss: {best_loss[2]:>2.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch   1 ( 31%), Mean Accuracy: 0.50693, Mean Loss: 0.69655\n",
      "Train epoch   1 ( 62%), Mean Accuracy: 0.52622, Mean Loss: 0.68803\n",
      "Train epoch   1 ( 92%), Mean Accuracy: 0.53491, Mean Loss: 0.68143\n",
      "Validation epoch   1 (100%), Accucary: 0.51795, Mean Loss: 0.65910 \n",
      "\n",
      "Train epoch   2 ( 31%), Mean Accuracy: 0.56634, Mean Loss: 0.65698\n",
      "Train epoch   2 ( 92%), Mean Accuracy: 0.60754, Mean Loss: 0.64442\n",
      "Validation epoch   2 (100%), Accucary: 0.56923, Mean Loss: 0.61121 \n",
      "\n",
      "Train epoch   3 ( 31%), Mean Accuracy: 0.64179, Mean Loss: 0.63279\n",
      "Train epoch   3 ( 62%), Mean Accuracy: 0.63928, Mean Loss: 0.62444\n",
      "Train epoch   3 ( 92%), Mean Accuracy: 0.63831, Mean Loss: 0.61956\n",
      "Validation epoch   3 (100%), Accucary: 0.51282, Mean Loss: 0.65215 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-476a1e661945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_batch_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-02178bff3fe4>\u001b[0m in \u001b[0;36mtrainLoop\u001b[0;34m(epochs, lr, wd, print_batch_p, early_patience)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_batch_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0maccs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-02178bff3fe4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epoch, print_batch_p)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-02178bff3fe4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainLoop(epochs=100, lr=0.001, wd=1e-6, print_batch_p=0.33, early_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
